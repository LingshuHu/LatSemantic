for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("hoax", df$text, ignore.case = T)& #frontlines?; 5G; fight; hoax
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/hoax_week1-10.csv") # frontline; 5g; fight; hoax
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/hoax_week1-10_texts.csv")
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("flu", df$text, ignore.case = T)& #frontlines?; 5G; fight; hoax; flu
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/flu_week1-10.csv") # frontline; 5g; fight; hoax; flu
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/flu_week1-10_texts.csv")
## read data
df <- read.csv("results/simi_usa01-52_word2vec_non-neg.csv")
View(df)
weeks <- lapply(1:52, paste0, "week", x)
weeks <- lapply(1:52, function(x) paste0, "week", x)
weeks <- lapply(1:52, function(x) paste0("week", x))
weeks <- unlist(lapply(1:52, function(x) paste0("week", x)))
colnames(df) <- c("word1", "word2", weeks)
View(df)
df[df == 9, ]
df[df == 9, ] <- NA
View(df)
df[df == 	9.0000000, ] <- NA
df[df == 	9] <- NA
View(df)
avg_simi <- apply(df[,3:54], 1, ,mean, na.rm = T)
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
avg_simi <- apply(df[,3:54], 2, mean, na.rm = T)
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
df <- cbind(df, avg_simi)
View(df)
?order
df2 <- df[order(df$avg_simi, decreasing = T), ]
View(df2)
df2[1:10, c("word1", "word2", "avg_simi")]
df2[1:20, c("word1", "word2", "avg_simi")]
df2[21:40, c("word1", "word2", "avg_simi")]
View(df)
df_met <- df[1:101, ]
View(df_met)
df_met <- df_met[order(df_met$avg_simi, decreasing = T), ]
df_met[1:40, c("word1", "word2", "avg_simi")]
library(ggplot2)
df_met_long <- tidyr::gather(df_met, 3:54)
df_met_long <- tidyr::gather(df_met, key = "key", value = "value",  3:54)
View(df_met_long)
ggplot(df_met_long, aes(x = key, y = value, color = word1)) + geom_line()
View(df_met_long)
ggplot(df_met_long, aes(x = key, y = value), color = word1) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1), color = word1) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long <- tidyr::gather(df_met[1:10, ], key = "key", value = "value",  3:54)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long$key <- sub("week", "", df_met_long)
df_met_long$key <- as.integer(df_met_long$key)
View(df_met_long)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
View(df_met_long)
df_met_long$key <- sub("week", "", df_met_long$key)
View(df_met_long)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line(color = group)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth()
?geom_smooth
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:10, ], key = "key", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long$weeks <- sub("week", "", df_met_long$weeks)
df_met_long$weeks <- as.integer(df_met_long$weeks)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
## read data
df <- read.csv("results/simi_usa01-52_word2vec_non-neg.csv")
weeks <- unlist(lapply(1:52, function(x) paste0("week", x)))
colnames(df) <- c("word1", "word2", weeks)
df[df == 9] <- NA
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
df <- cbind(df, avg_simi)
df_met <- df[1:101, ]
df_met <- df_met[order(df_met$avg_simi, decreasing = T), ]
df_met[1:40, c("word1", "word2", "avg_simi")]
## visualize
library(ggplot2)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$weeks <- sub("week", "", df_met_long$weeks)
df_met_long$weeks <- as.integer(df_met_long$weeks)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa")
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa", full.names = T)
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa", full.names = T)
###################### find frequency of metaphor and misinfo words ################
install.packages("jsonlite")
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
kw <- jsonlite::fromJSON("data/all_keywords_pairs.json", flatten = T)
View(kw)
kw <- unique(kw[,1])
tt <- readLines(files[1])
stringr::str_count(tt, "\\<homefront//>")
stringr::str_count(tt, "\\<covid19//>")
tt[1:5]
stringr::str_count(tt[1], "\\<covid19//>")
stringr::str_count(tt[1], "\\<covid19//>")
stringr::str_count(tt[1], "covid19")
stringr::str_count(tt[1], "\\<covid19\\>")
?str_count
stringr::str_count(tt[1], boundary("covid19"))
stringr::str_count(tt[1], stringr::boundary("covid19"))
stringr::str_count(tt[1], stringr::boundary("word"))
stringr::str_count(tt[1], "covid19", stringr::boundary("word"))
stringr::str_count(tt[1], "covid")
stringr::str_count(tt[1], "co")
## function to count words matches in a dictionary
word_match <- function(x, dict) {
if (is.character(x)) {
## this removes URLs
x <- gsub("https?://\\S+|@\\S+", "", x)
x <- tokenizers::tokenize_words(
x, lowercase = TRUE, strip_punct = TRUE, strip_numeric = FALSE
)
}
word_count <- function(token) {
total_words_count <- length(token)
med_words_count <- sum(dict$value[match(token, dict$word)], na.rm = TRUE)
med_words_ratio <- med_words_count/total_words_count
data.frame(total_words_count = total_words_count,
med_words_count = med_words_count,
med_words_ratio = med_words_ratio,
stringsAsFactors = FALSE)
}
count <- lapply(x, word_count)
count <- do.call("rbind", count)
}
kw <- data.frame(word = kw, value = repeat(1, length(kw)))
kw <- data.frame(word = kw, value = rep(1, length(kw)))
View(kw)
frq <- word_match(tt, kw)
View(frq)
View(frq)
tt[2]
View(kw)
?str_c
tt1 <- stringr::str_c(tt[1:5], collapse = " ")
tt1
tt[1:5]
tt1 <- stringr::str_c(tt, collapse = " ")
frq <- lapply(kw, function(i) word_match(tt1, i))
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
## function to count words matches in a dictionary
word_match <- function(x, dict) {
if (is.character(x)) {
## this removes URLs
x <- gsub("https?://\\S+|@\\S+", "", x)
x <- tokenizers::tokenize_words(
x, lowercase = TRUE, strip_punct = TRUE, strip_numeric = FALSE
)
}
word_count <- function(token) {
total_words_count <- length(token)
med_words_count <- sum(dict$value[match(token, dict$word)], na.rm = TRUE)
med_words_ratio <- med_words_count/total_words_count
data.frame(total_words_count = total_words_count,
med_words_count = med_words_count,
stringsAsFactors = FALSE)
}
#count <- lapply(x, word_count)
#count <- do.call("rbind", count)
count <- word_count(x)
}
frq <- lapply(kw, function(i) word_match(tt1, i))
seq_along(kw$word)
frq <- vector("list", length = nrow(kw))
for (i in seq_along(kw$word)) {
frq[[i]] <- word_match(tt1, kw[i, ])
}
View(frq)
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
fs <- list.files("results/usa", full.names = T)
freq <- vector("list", length = length(fs))
?read.csv
freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df), ] <- 0
df$rate <- df$frequency/df$total_words
freq[[i]] <- df[sort(df$rate, decreasing = T), ]
}
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- df$frequency/df$total_words
freq[[i]] <- df[sort(df$rate, decreasing = T), ]
}
df <- read.csv(fs[1], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- df$frequency/df$total_words
View(df)
df <- read.csv(fs[1], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- df$frequency/df$total_words
freq[[i]] <- df[sort(df$rate, decreasing = T), ]
View(df)
View(freq)
df[sort(df$rate, decreasing = T), ]
df[order(df$rate, decreasing = T), ]
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
kw <- jsonlite::fromJSON("data/all_keywords_pairs.json", flatten = T)
kw
met <- kw[1:101, 1]
met <- unique(met)
mis1 <- kw[102:179, 1]
mis2 <- kw[102:179, 1]
mis2 <- kw[102:179, 2]
mis2 <- kw[116:179, 2]
mis <- unique(c(mis1, mis2))
simi <- read.csv("results/simi_usa01-52_word2vec_non-neg.csv", stringsAsFactors = F)
View(simi)
paste0("week", 1:52)
colnames(simi) <- c("word1", "word2", paste0("week", 1:52))
View(simi)
View(kw)
View(df)
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[, paste0("week", i)], by = c("words", "word1"))
str(simi)
str(met_df)
met_df <- dplyr::left_join(met_df, simi[, c("word1", paste0("week", i))], by = c("words", "word1"))
simi[, c("word1", paste0("week", i))]
View(met_df)
met_df <- dplyr::left_join(met_df, simi[, c("word1", paste0("week", i))], by = c("words", "word1"))
met_df <- dplyr::left_join(met_df, simi[, c("word1", paste0("week", i))], by.x = "words", by.y = "word1")
met_df <- dplyr::left_join(met_df, simi[, c("word1", paste0("week", i))], by = c("words" = "word1"))
View(met_df)
View(simi)
met_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[1], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", i))], by = c("words" = "word1"))
met_freq[[i]] <- met_df[order(df$rate, decreasing = T), ]
}
View(met_df)
met_freq <- do.call("cbind", met_freq)
View(met_freq)
met_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", i))], by = c("words" = "word1"))
met_freq[[i]] <- met_df[order(df$rate, decreasing = T), ]
}
met_freq <- do.call("cbind", met_freq)
View(met_freq)
met_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", i))], by = c("words" = "word1"))
met_freq[[i]] <- met_df[order(df$rate, decreasing = T), ]
}
View(met_freq)
df <- read.csv(fs[2], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", 2))], by = c("words" = "word1"))
View(met_df)
x <- met_df[order(df$rate, decreasing = T), ]
View(x)
met_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", i))], by = c("words" = "word1"))
met_freq[[i]] <- met_df[order(met_df$rate, decreasing = T), ]
}
met_freq <- do.call("cbind", met_freq)
View(met_freq)
View(met_df)
met_freq <- vector("list", length = length(fs))
mis_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- dplyr::left_join(met_df, simi[1:101, c("word1", paste0("week", i))], by = c("words" = "word1"))
colnames(met_df)[6] <- "similarity"
met_freq[[i]] <- met_df[order(met_df$rate, decreasing = T), ]
}
met_freq <- do.call("cbind", met_freq)
View(met_freq)
write.csv(met_freq, "results/top_met_words_by_weeks.csv")
write.csv(met_freq, "results/top_met_words_by_weeks.csv", row.names = F)
#######################
## similarity among 5 top words
simi <- read.csv("results/simi_usa01-52_word2vec_non-neg_all_connections.csv", stringsAsFactors = F)
View(simi)
colnames(simi) <- c("word1", "word2", paste0("week", 1:52))
5*4*3*2
simi$pairs <- paste0(simi$word1, simi$word2, sep = "-")
View(simi)
simi$pairs[1:5]
simi$pairs <- paste(simi$word1, simi$word2, sep = "-")
simi$pairs[1:5]
139*139
top_met_simis <- vector("list", length = length(fs))
#mis_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- met_df[order(met_df$rate, decreasing = T), ]
w1 <- vector("list", length = 5*5)
for (x in seq_along(met_df$words[1:5])) {
w2 <- vector("list", length = 5*5)
for (y in seq_along(met_df$words[1:5])) {
w2[[y]] <- paste(w1, w2, sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = w1)
pairs <- dplyr::left_join(met_df, simi[, c("pairs", paste0("week", i))], by = "pairs")
#colnames(pairs)[6] <- "similarity"
top_met_simis[[i]] <- pairs
}
df <- read.csv(fs[1], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- met_df[order(met_df$rate, decreasing = T), ]
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words[1:5])) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[1:5])) {
w2[[y]] <- paste(w1, w2, sep = "-")
}
w1[[x]] <- unlist(w2)
}
met_df <- met_df[1:5, ]
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words)) {
w2[[y]] <- paste(met_df$words[x], met_df$words[y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = w1)
View(pairs)
pairs <- data.frame(pairs = unlist(w1))
View(pairs)
pairs <- dplyr::left_join(met_df, simi[, c("pairs", paste0("week", 1))], by = "pairs")
pairs <- dplyr::left_join(pairs, simi[, c("pairs", paste0("week", 1))], by = "pairs")
View(pairs)
5*4*3*2
5*5
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[5-x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
View(pairs)
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[6-x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
View(pairs)
met_df$words
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
View(pairs)
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[x:5][y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
View(pairs)
pairs <- dplyr::left_join(pairs, simi[, c("pairs", paste0("week", 1))], by = "pairs")
top_met_simis <- vector("list", length = length(fs))
#mis_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- met_df[order(met_df$rate, decreasing = T), ]
met_df <- met_df[1:5, ]
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[x:5][y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
pairs <- dplyr::left_join(pairs, simi[, c("pairs", paste0("week", 1))], by = "pairs")
#colnames(pairs)[6] <- "similarity"
top_met_simis[[i]] <- pairs
}
top_met_simis <- do.call("cbind", top_met_simis)
View(top_met_simis)
top_met_simis <- vector("list", length = length(fs))
#mis_freq <- vector("list", length = length(fs))
for (i in seq_along(fs)) {
df <- read.csv(fs[i], stringsAsFactors = F)
df[is.na(df)] <- 0
df$rate <- 100*df$frequency/df$total_words
met_df <- subset(df, words %in% met)
met_df <- met_df[order(met_df$rate, decreasing = T), ]
met_df <- met_df[1:5, ]
w1 <- vector("list", length = 5)
for (x in seq_along(met_df$words)) {
w2 <- vector("list", length = 5)
for (y in seq_along(met_df$words[x:5])) {
w2[[y]] <- paste(met_df$words[x], met_df$words[x:5][y], sep = "-")
}
w1[[x]] <- unlist(w2)
}
pairs <- data.frame(pairs = unlist(w1))
pairs <- dplyr::left_join(pairs, simi[, c("pairs", paste0("week", i))], by = "pairs")
#colnames(pairs)[6] <- "similarity"
top_met_simis[[i]] <- pairs
}
top_met_simis <- do.call("cbind", top_met_simis)
View(top_met_simis)
write.csv(top_met_simis, "results/top_met_words_by_weeks_all_connections.csv", row.names = F)
