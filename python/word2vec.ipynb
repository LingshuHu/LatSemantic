{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "import gensim\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words2 = stop_words[35:] # include i, we, they, it, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # remove link, user and special characters\n",
    "    #text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n",
    "    # Remove link\n",
    "    text = re.sub(r'https?:\\S+|http?:\\S', ' ', str(text).lower()).strip()\n",
    "    # change all to covid19\n",
    "    text = re.sub(r'(covid|c|cv)(\\s+|\\-|\\_)19', 'covid19 ', text)\n",
    "    text = re.sub(r'covid(\\s+|\\W|$)', 'covid19 ', text)\n",
    "    text = re.sub(r'coronavirus|caronavirus', 'covid19 ', text)\n",
    "    text = re.sub(r'corona', 'covid19 ', text)\n",
    "    text = re.sub(r'rona', 'covid19 ', text)\n",
    "    text = re.sub(r'c\\-19|c19|cv19|cv\\-19', 'covid19 ', text)\n",
    "    # remove puctuations and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # remove first space\n",
    "    text = re.sub(r'^\\s+', '', text)\n",
    "    # Removing prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words2:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump covid19 covid19 covid19 covid19 covid19 covid19 covid19 covid19 virus covid19'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"@trump covid 19 and! covid-19. and, ...covid_19, and #covid? c19 c 19 cv-19 covid19-virus coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "#df = pd.read_csv(\"../data/covid_usa_jan24-mar10_21pm-23pm_onethird_text.csv\", encoding = \"ISO-8859-1\")\n",
    "df = pd.read_csv(\"../data/covid_usa_mar11-may25_21pm-23pm_onethird_text.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 4s, sys: 208 ms, total: 3min 5s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clean texts\n",
    "textp = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check weird words\n",
    "for i in range(len(textp)):\n",
    "    if textp[i].find(\"covid19virus\")>-1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.97 s, sys: 696 ms, total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in textp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 88 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# obtain words frequency\n",
    "# less effecient\n",
    "all_words = []\n",
    "for tweet in range(len(documents)):\n",
    "    for word in documents[tweet]:\n",
    "        all_words.append(word)\n",
    "        \n",
    "c = Counter(all_words)\n",
    "c200 = c.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.14 s, sys: 716 ms, total: 9.85 s\n",
      "Wall time: 9.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# obtain words frequency\n",
    "words = []\n",
    "for t in textp:\n",
    "    words += t.split()\n",
    "\n",
    "c = Counter(words)\n",
    "c200 = c.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/mar11-may25_common_words200.txt\", \"w\") as fp:\n",
    "    json.dump(c200, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(size=200, \n",
    "                                            window=5, \n",
    "                                            min_count=10, \n",
    "                                            workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 56605\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(documents)\n",
    "words = w2v_model.wv.vocab.keys()\n",
    "#print(len(w2v_model.wv[\"u\"]))\n",
    "#print(list(w2v_model.wv.vocab.items())[0:5])\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 41s, sys: 11.7 s, total: 48min 53s\n",
      "Wall time: 14min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1226241288, 1325684512)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"mar13-may25_w2vmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingshu/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('coronavirus', 0.8685397505760193),\n",
       " ('virus', 0.5763905048370361),\n",
       " ('also', 0.48496437072753906),\n",
       " ('amp', 0.4203587770462036),\n",
       " ('disease', 0.3924241065979004),\n",
       " ('many', 0.3548542559146881),\n",
       " ('means', 0.3501068949699402),\n",
       " ('cancer', 0.3478909432888031),\n",
       " ('cv', 0.32926028966903687),\n",
       " ('secondary', 0.3270239233970642)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[1, 0, -1]], [[-1,-1, 0]])\n",
    "type([[1, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model1 = gensim.models.Word2Vec.load(\"jan24-mar12_w2vmodel.w2v\")\n",
    "w2v_model2 = gensim.models.Word2Vec.load(\"mar13-may25_w2vmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi_compare(word1, word2):\n",
    "    wd11 = w2v_model1.wv[word1]\n",
    "    wd12 = w2v_model1.wv[word2]\n",
    "    wd21 = w2v_model2.wv[word1]\n",
    "    wd22 = w2v_model2.wv[word2]\n",
    "    print(\"jan24-mar12: \", cosine_similarity([wd11], [wd12]))\n",
    "    print(\"mar13-may25: \", cosine_similarity([wd21], [wd22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan24-mar12:  [[-0.02310156]]\n",
      "mar13-may25:  [[-0.00771967]]\n"
     ]
    }
   ],
   "source": [
    "# china, cases, death, asian, black, distance, hoax\n",
    "# metaphors, distrust, misinformation\n",
    "\n",
    "\n",
    "simi_compare(\"covid19\", \"mistrust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01677959]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd1 = w2v_model.wv[\"covid19\"]\n",
    "wd2 = w2v_model.wv[\"china\"]\n",
    "cosine_similarity([wd1], [wd2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coronavirus', 1), ('covid19', 2), ('trump', 3), ('people', 4), ('health', 5), ('amp', 6), ('us', 7), ('virus', 8), ('china', 9), ('cases', 10)]\n",
      "Total words 101545\n",
      "CPU times: user 9.04 s, sys: 3.64 ms, total: 9.04 s\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.textp)\n",
    "print(list(tokenizer.word_index.items())[0:10])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE)) #Return a new array of given shape and type, filled with zeros.\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in w2v_model.wv:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)\n",
    "print(len(embedding_matrix[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
