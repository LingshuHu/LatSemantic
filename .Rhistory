for (i in seq_along(nrow(wordpairs))) {
vecs <- dfm[, c(wordpairs[[1]][i], wordpairs[[2]][i])]
s[i] <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
}
return(s)
}
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(nrow(wordpairs))) {
vecs <- dfm[, c(wordpairs[[1]][i], wordpairs[[2]][i])]
s[i] <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
}
return(s)
}
covid_s <- multi_simi(covid_pair, dv1)
View(covid_s)
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(nrow(wordpairs))) {
vecs <- dfm[, c(wordpairs[[1]][i], wordpairs[[2]][i])]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[i] <- obj@x[2]
}
return(s)
}
covid_s <- multi_simi(covid_pair, dv1)
View(covid_s)
5g %in% dv1
"5g" %in% dv1
dv1
View(dv1)
dv1@Dimnames
dv1@Dimnames$features
dv1[, "radiation"]
dv1[, "radiatisdfsd"]
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(nrow(wordpairs))) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wordpairs[[1]][i], wordpairs[[2]][i])]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[i] <- obj@x[2]
} else {s[i] <- 9}
}
return(s)
}
covid_s <- multi_simi(covid_pair, dv1)
View(covid_s)
covid_s <- multi_simi(race_pair, dv1)
View(covid_s)
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(nrow(wordpairs))) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
return(s)
}
covid_s <- multi_simi(race_pair, dv1)
View(covid_s)
s <- vector("list", length = nrow(race_pair))
for (i in seq_along(nrow(race_pair))) {
wd1 <- race_pair[[1]][i]
wd2 <- race_pair[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
s <- vector("list", length = nrow(race_pair))
for (i in seq_along(nrow(race_pair))) {
wd1 <- race_pair[[1]][i]
wd2 <- race_pair[[2]][i]
if (wd1 %in% dv1@Dimnames$features & wd2 %in% dv1@Dimnames$features) {
vecs <- dv1[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
unlist(s)
seq_along(nrow(race_pair))
seq_along(race_pair)
?seq_along
nrow(race_pair)
seq_along(36)
seq_along(length(36))
seq_along(length=36)
seq_along(from =1, to=36)
seq_along(race_pair[[1]])
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(nrow(wordpairs[[1]]))) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
return(s)
}
covid_s <- multi_simi(race_pair, dv1)
View(covid_s)
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(wordpairs[[1]])) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
return(s)
}
covid_s <- multi_simi(race_pair, dv1)
View(covid_s)
covid_s <- unlist(multi_simi(race_pair, dv1))
race_pair_simi <- data.frame(race_pair, before = covid_s)
View(race_pair_simi)
other_pair <- rbind(c(1,2), c(2,3))
other_pair <- data.frame(rbind(c(1,2), c(3,4)))
View(other_pair)
View(other_pair)
covid_pair <- tidyr::crossing(X1 = "covid19", X2 = covidmis)
race_pair <- tidyr::crossing(X1 = race, X2 = racemis)
other_pair <- data.frame(rbind(c("mandatory", "vaccine"), c("overcount", "death"), c("dioxide", "mask"), c("lab", "china")))
all_pairs <- rbind(covid_pair, race_pair, other_pair)
View(all_pairs)
all_pair <- rbind(covid_pair, race_pair, other_pair)
df2 <- rtweet::read_twitter_csv("data/covid_usa_jan24-mar10_21pm-23pm_onethird_text_clean.csv")
dv2 <- quanteda::dfm(df2$text.1)
View(df2)
## read data
df1 <- rtweet::read_twitter_csv("data/covid_usa_jan24-mar10_21pm-23pm_onethird_text_clean.csv")
df2 <- rtweet::read_twitter_csv("data/covid_usa_jan24-mar10_21pm-23pm_onethird_text_clean.csv")
dv1 <- quanteda::dfm(df1$textp)
dv2 <- quanteda::dfm(df2$textp)
df2 <- rtweet::read_twitter_csv("data/covid_usa_mar11-may25_21pm-23pm_onethird_text_clean.csv")
dv2 <- quanteda::dfm(df2$textp)
## calculate similarities between all pairs
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(wordpairs[[1]])) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
return(s)
}
all_s1 <- unlist(multi_simi(all_pair, dv1))
all_s2 <- unlist(multi_simi(all_pair, dv2))
all_pair_simi <- data.frame(race_pair, before = all_s1, after = all_s2)
all_pair_simi <- data.frame(all_pair, before = all_s1, after = all_s2)
View(all_pair_simi)
write.csv(all_pair_simi, "results/covid_misinfo_onthot.csv")
write.csv(all_pair_simi, "results/covid_misinfo_onehot.csv")
## read data
df1 <- rtweet::read_twitter_csv("data/covid_usa_jan24-mar10_21pm-23pm_onethird_text_clean.csv")
df2 <- rtweet::read_twitter_csv("data/covid_usa_mar11-may25_21pm-23pm_onethird_text_clean.csv")
dv1 <- quanteda::dfm(df1$textp)
dv2 <- quanteda::dfm(df2$textp)
metaphor <- readLines("../data/keywords1.txt")
metaphor <- readLines("data/keywords1.txt")
metaphor
metaphor <- metaphor[1]
metaphor <- gsub('\\n+|\\s+', '', tolower(metaphor))
metaphor
metaphor <- strsplit(metaphor, split = ",")
View(metaphor)
metaphor <- unlist(strsplit(metaphor, split = ","))
metaphor <- readLines("data/keywords1.txt")
metaphor <- metaphor[1]
metaphor <- gsub('\\n+|\\s+', '', tolower(metaphor))
metaphor <- unlist(strsplit(metaphor, split = ","))
meta_pair <- tidyr::crossing(X1 = "covid19", X2 = metaphor)
View(meta_pair)
## calculate similarities between all pairs
multi_simi <- function(wordpairs, dfm) {
s <- vector("list", length = nrow(wordpairs))
for (i in seq_along(wordpairs[[1]])) {
wd1 <- wordpairs[[1]][i]
wd2 <- wordpairs[[2]][i]
if (wd1 %in% dfm@Dimnames$features & wd2 %in% dfm@Dimnames$features) {
vecs <- dfm[, c(wd1, wd2)]
obj <- quanteda::textstat_simil(vecs,  method = "cosine", margin = "features")
s[[i]] <- obj@x[2]
} else {s[[i]] <- 9}
}
return(s)
}
meta_s1 <- unlist(multi_simi(meta_pair, dv1))
meta_s2 <- unlist(multi_simi(meta_pair, dv2))
meta_pair_simi <- data.frame(meta_pair, before = meta_s1, after = meta_s2)
write.csv(meta_pair_simi, "results/covid_metaphor_onehot.csv")
df1 <- rtweet::read_twitter_csv("data/covid_usa_jan24-mar10_21pm-23pm_onethird_text_clean.csv")
grepl("army", "this is army")
grepl("army|covid", "this is army and covid")
grepl("army&]covid", "this is army and covid")
grepl("army&covid", "this is army and covid")
dfs <- df1[grepl("army", df1$textp)&grepl("covid19", df1$textp),]
dfs$textp[1:10]
dfs$text[1:10]
writeLines(dfs$text, "results/army&covid.txt")
txt <- dfs$text
txt <- data.frame(id = 1:403, text = dfs$text)
write.csv(txt, "results/army&covid.csv")
dfs<- dfs[!duplicated(dfs$textp), ]
txt <- data.frame(id = 1:203, text = dfs$text)
write.csv(txt, "results/army&covid.csv")
dfs <- df1[grepl("battles", df1$textp)&grepl("covid19", df1$textp),]
dfs <- dfs[!duplicated(dfs$textp), ]
txt <- data.frame(id = 1:30, text = dfs$text)
write.csv(txt, "results/battles&covid.csv")
dfs <- df1[grepl("radiation", df1$textp)&grepl("covid19", df1$textp),]
dfs <- dfs[!duplicated(dfs$textp), ]
df2 <- rtweet::read_twitter_csv("data/covid_usa_mar11-may25_21pm-23pm_onethird_text_clean.csv")
dfs <- df1[grepl("hoax", df1$textp)&grepl("covid19", df1$textp),]
dfs <- dfs[!duplicated(dfs$textp), ]
txt <- data.frame(id = nrow(dfs), text = dfs$text)
write.csv(txt, "results/hoax&covid.csv")
#####
files <- list.files("data_week/usa", full.names = T)
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("frontlines?", df$text, ignore.case = T)&
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/frontline_week1-10.csv")
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/frontline_week1-10_texts.csv")
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("5G", df$text, ignore.case = T)& #frontlines?
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/5g_week1-10.csv") # frontline
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/5g_week1-10_texts.csv")
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("fight", df$text, ignore.case = T)& #frontlines?; 5G
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/fight_week1-10.csv") # frontline; 5g; fight
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/fight_week1-10_texts.csv")
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("hoax", df$text, ignore.case = T)& #frontlines?; 5G; fight; hoax
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/hoax_week1-10.csv") # frontline; 5g; fight; hoax
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/hoax_week1-10_texts.csv")
key_sample <- vector("list", length = length(files))
for (i in seq_along(files)) {
df <- rtweet::read_twitter_csv(files[i])
keydf <- df[grepl("flu", df$text, ignore.case = T)& #frontlines?; 5G; fight; hoax; flu
grepl("covid19|covid|coronavirus", df$text, ignore.case = T),]
if (nrow(keydf) > 1000) {
set.seed(123)
keydf <- dplyr::sample_n(keydf, 1000)
}
key_sample[[i]] <- keydf
}
key_sample <- do.call("rbind", key_sample)
key_sample <- key_sample[!duplicated(key_sample$text), ]
rtweet::write_as_csv(key_sample, "sample_texts/flu_week1-10.csv") # frontline; 5g; fight; hoax; flu
rtweet::write_as_csv(key_sample[, 1:5], "sample_texts/flu_week1-10_texts.csv")
## read data
df <- read.csv("results/simi_usa01-52_word2vec_non-neg.csv")
View(df)
weeks <- lapply(1:52, paste0, "week", x)
weeks <- lapply(1:52, function(x) paste0, "week", x)
weeks <- lapply(1:52, function(x) paste0("week", x))
weeks <- unlist(lapply(1:52, function(x) paste0("week", x)))
colnames(df) <- c("word1", "word2", weeks)
View(df)
df[df == 9, ]
df[df == 9, ] <- NA
View(df)
df[df == 	9.0000000, ] <- NA
df[df == 	9] <- NA
View(df)
avg_simi <- apply(df[,3:54], 1, ,mean, na.rm = T)
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
avg_simi <- apply(df[,3:54], 2, mean, na.rm = T)
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
df <- cbind(df, avg_simi)
View(df)
?order
df2 <- df[order(df$avg_simi, decreasing = T), ]
View(df2)
df2[1:10, c("word1", "word2", "avg_simi")]
df2[1:20, c("word1", "word2", "avg_simi")]
df2[21:40, c("word1", "word2", "avg_simi")]
View(df)
df_met <- df[1:101, ]
View(df_met)
df_met <- df_met[order(df_met$avg_simi, decreasing = T), ]
df_met[1:40, c("word1", "word2", "avg_simi")]
library(ggplot2)
df_met_long <- tidyr::gather(df_met, 3:54)
df_met_long <- tidyr::gather(df_met, key = "key", value = "value",  3:54)
View(df_met_long)
ggplot(df_met_long, aes(x = key, y = value, color = word1)) + geom_line()
View(df_met_long)
ggplot(df_met_long, aes(x = key, y = value), color = word1) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1), color = word1) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long <- tidyr::gather(df_met[1:10, ], key = "key", value = "value",  3:54)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
df_met_long$key <- sub("week", "", df_met_long)
df_met_long$key <- as.integer(df_met_long$key)
View(df_met_long)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
View(df_met_long)
df_met_long$key <- sub("week", "", df_met_long$key)
View(df_met_long)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1)) + geom_line(color = group)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth()
?geom_smooth
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:10, ], key = "key", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "key", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) + geom_line()
ggplot(df_met_long, aes(x = key, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$key <- sub("week", "", df_met_long$key)
df_met_long$key <- as.integer(df_met_long$key)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
df_met_long$weeks <- sub("week", "", df_met_long$weeks)
df_met_long$weeks <- as.integer(df_met_long$weeks)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
## read data
df <- read.csv("results/simi_usa01-52_word2vec_non-neg.csv")
weeks <- unlist(lapply(1:52, function(x) paste0("week", x)))
colnames(df) <- c("word1", "word2", weeks)
df[df == 9] <- NA
avg_simi <- apply(df[,3:54], 1, mean, na.rm = T)
df <- cbind(df, avg_simi)
df_met <- df[1:101, ]
df_met <- df_met[order(df_met$avg_simi, decreasing = T), ]
df_met[1:40, c("word1", "word2", "avg_simi")]
## visualize
library(ggplot2)
df_met_long <- tidyr::gather(df_met[1:5, ], key = "weeks", value = "value",  3:54)
df_met_long$weeks <- sub("week", "", df_met_long$weeks)
df_met_long$weeks <- as.integer(df_met_long$weeks)
ggplot(df_met_long, aes(x = weeks, y = value, group = word1, color = word1)) +
geom_point() + geom_smooth(se = F)
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa")
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa", full.names = T)
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
files <- list.files("data_week_text/usa", full.names = T)
###################### find frequency of metaphor and misinfo words ################
install.packages("jsonlite")
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
source('~/Documents/Projects/latent_semantic/R/analyze_similarity.R')
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
kw <- jsonlite::fromJSON("data/all_keywords_pairs.json", flatten = T)
View(kw)
kw <- unique(kw[,1])
tt <- readLines(files[1])
stringr::str_count(tt, "\\<homefront//>")
stringr::str_count(tt, "\\<covid19//>")
tt[1:5]
stringr::str_count(tt[1], "\\<covid19//>")
stringr::str_count(tt[1], "\\<covid19//>")
stringr::str_count(tt[1], "covid19")
stringr::str_count(tt[1], "\\<covid19\\>")
?str_count
stringr::str_count(tt[1], boundary("covid19"))
stringr::str_count(tt[1], stringr::boundary("covid19"))
stringr::str_count(tt[1], stringr::boundary("word"))
stringr::str_count(tt[1], "covid19", stringr::boundary("word"))
stringr::str_count(tt[1], "covid")
stringr::str_count(tt[1], "co")
## function to count words matches in a dictionary
word_match <- function(x, dict) {
if (is.character(x)) {
## this removes URLs
x <- gsub("https?://\\S+|@\\S+", "", x)
x <- tokenizers::tokenize_words(
x, lowercase = TRUE, strip_punct = TRUE, strip_numeric = FALSE
)
}
word_count <- function(token) {
total_words_count <- length(token)
med_words_count <- sum(dict$value[match(token, dict$word)], na.rm = TRUE)
med_words_ratio <- med_words_count/total_words_count
data.frame(total_words_count = total_words_count,
med_words_count = med_words_count,
med_words_ratio = med_words_ratio,
stringsAsFactors = FALSE)
}
count <- lapply(x, word_count)
count <- do.call("rbind", count)
}
kw <- data.frame(word = kw, value = repeat(1, length(kw)))
kw <- data.frame(word = kw, value = rep(1, length(kw)))
View(kw)
frq <- word_match(tt, kw)
View(frq)
View(frq)
tt[2]
View(kw)
?str_c
tt1 <- stringr::str_c(tt[1:5], collapse = " ")
tt1
tt[1:5]
tt1 <- stringr::str_c(tt, collapse = " ")
frq <- lapply(kw, function(i) word_match(tt1, i))
###################### find frequency of metaphor and misinfo words ################
#install.packages("jsonlite")
## function to count words matches in a dictionary
word_match <- function(x, dict) {
if (is.character(x)) {
## this removes URLs
x <- gsub("https?://\\S+|@\\S+", "", x)
x <- tokenizers::tokenize_words(
x, lowercase = TRUE, strip_punct = TRUE, strip_numeric = FALSE
)
}
word_count <- function(token) {
total_words_count <- length(token)
med_words_count <- sum(dict$value[match(token, dict$word)], na.rm = TRUE)
med_words_ratio <- med_words_count/total_words_count
data.frame(total_words_count = total_words_count,
med_words_count = med_words_count,
stringsAsFactors = FALSE)
}
#count <- lapply(x, word_count)
#count <- do.call("rbind", count)
count <- word_count(x)
}
frq <- lapply(kw, function(i) word_match(tt1, i))
seq_along(kw$word)
frq <- vector("list", length = nrow(kw))
for (i in seq_along(kw$word)) {
frq[[i]] <- word_match(tt1, kw[i, ])
}
View(frq)
