{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "import gensim\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = WordNetLemmatizer()\n",
    "#rona, c19, c-19, cv19, cv-19, corona\n",
    "\n",
    "def text_process(text):\n",
    "    # Remove all the special characters\n",
    "    #document = re.sub(r'\\W', ' ', str(text))\n",
    "    document = re.sub(r'https?:\\S+|http?:\\S', '', str(text))\n",
    "    # remove all single characters\n",
    "    #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    #document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    # remove punctuations\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # remove first space\n",
    "    document = re.sub(r'^\\s+', '', document)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # change covid 19 to covid19\n",
    "    document = re.sub(r'covid(\\s+|\\-|\\_)19', 'covid19', document)\n",
    "    # change covid to covid19\n",
    "    document = re.sub(r'covid(\\s+|\\W|$)', 'covid19', document)\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid 19 covid 19 covid 19 covid'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"covid 19 and covid-19. and covid_19, and #covid?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19 and covid19 and covid19 and covid19'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process(\"covid 19 and covid-19. and covid_19, and #covid?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv(\"../data/covid_usa_jan24-mar12_21pm-23pm_onethird_text.csv\", encoding = \"ISO-8859-1\")\n",
    "#df = pd.read_csv(\"../data/covid_usa_mar13-may25_21pm-23pm_onethird_text.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingshu/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.textp = df.text.apply(lambda x: preprocess(x))\n",
    "df.textp = df.textp.apply(lambda x: text_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 s, sys: 572 ms, total: 6.31 s\n",
      "Wall time: 6.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in df.textp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for t in df.textp:\n",
    "    words += t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "c50 = c.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jan24-mar12_common_words50.txt\", \"w\") as fp:\n",
    "    json.dump(c50, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(size=200, \n",
    "                                            window=5, \n",
    "                                            min_count=10, \n",
    "                                            workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 56605\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(documents)\n",
    "words = w2v_model.wv.vocab.keys()\n",
    "#print(len(w2v_model.wv[\"u\"]))\n",
    "#print(list(w2v_model.wv.vocab.items())[0:5])\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 41s, sys: 11.7 s, total: 48min 53s\n",
      "Wall time: 14min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1226241288, 1325684512)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"mar13-may25_w2vmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingshu/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('coronavirus', 0.8685397505760193),\n",
       " ('virus', 0.5763905048370361),\n",
       " ('also', 0.48496437072753906),\n",
       " ('amp', 0.4203587770462036),\n",
       " ('disease', 0.3924241065979004),\n",
       " ('many', 0.3548542559146881),\n",
       " ('means', 0.3501068949699402),\n",
       " ('cancer', 0.3478909432888031),\n",
       " ('cv', 0.32926028966903687),\n",
       " ('secondary', 0.3270239233970642)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[1, 0, -1]], [[-1,-1, 0]])\n",
    "type([[1, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model1 = gensim.models.Word2Vec.load(\"jan24-mar12_w2vmodel.w2v\")\n",
    "w2v_model2 = gensim.models.Word2Vec.load(\"mar13-may25_w2vmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi_compare(word1, word2):\n",
    "    wd11 = w2v_model1.wv[word1]\n",
    "    wd12 = w2v_model1.wv[word2]\n",
    "    wd21 = w2v_model2.wv[word1]\n",
    "    wd22 = w2v_model2.wv[word2]\n",
    "    print(\"jan24-mar12: \", cosine_similarity([wd11], [wd12]))\n",
    "    print(\"mar13-may25: \", cosine_similarity([wd21], [wd22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan24-mar12:  [[-0.02310156]]\n",
      "mar13-may25:  [[-0.00771967]]\n"
     ]
    }
   ],
   "source": [
    "# china, cases, death, asian, black, distance, hoax\n",
    "# metaphors, distrust, misinformation\n",
    "\n",
    "\n",
    "simi_compare(\"covid19\", \"mistrust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01677959]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd1 = w2v_model.wv[\"covid19\"]\n",
    "wd2 = w2v_model.wv[\"china\"]\n",
    "cosine_similarity([wd1], [wd2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coronavirus', 1), ('covid19', 2), ('trump', 3), ('people', 4), ('health', 5), ('amp', 6), ('us', 7), ('virus', 8), ('china', 9), ('cases', 10)]\n",
      "Total words 101545\n",
      "CPU times: user 9.04 s, sys: 3.64 ms, total: 9.04 s\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.textp)\n",
    "print(list(tokenizer.word_index.items())[0:10])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE)) #Return a new array of given shape and type, filled with zeros.\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in w2v_model.wv:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)\n",
    "print(len(embedding_matrix[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
